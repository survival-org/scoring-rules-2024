[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Material",
    "section": "",
    "text": "Note\n\n\n\nThis sections refers to the results from the simulation experiments of Section 6.2 of the paper.\nUse properness_test.R to run the experiments. To ran the script for different sample sizes n, use the run_tests.sh script. The output results table (merged for all n) is available here.\n\n\nLoad libraries:\n\n\nCode\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(DT)\n\n\nLoad results, and show 10 randomly selected simulations outputs:\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_0.rds\")\n\nres[sample(1:nrow(res), 10), ] |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE)\n  ) |&gt;\n  formatRound(columns = 3:15, digits = c(rep(3,8), rep(4,5)))\n\n\n\n\n\n\nThe output columns are:\n\nsim: simulation number (k in the paper, in each simulation we draw m = 1000 times from each Weibull distribution)\nn: sample size (10 - 2500)\n{surv|pred|cens|}_{scale|shape} the scale and shape of the Weibull distributions for true, predicted and censoring times respectively\nprop_cens: proportion of censoring in each simulation\ntv_dist: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)\n{score}_diff: the mean score difference \\delta between true and predicted distribution. Scores used are the SBS at the 10, 50 and 90 quantiles of observed times, the ISBS from the 5 up to the 80 quantile (50 equidistant time points), the RCLL and the re-weighted rRCLL (RCLL** in the paper).\n{score}_se: the mean of two squared errors for the (I)SBS scores (using the true and the predicted survival distribution) across the m = 1000 distributions.\n\nCalculate violation stats:\n\n\nCode\nres = res |&gt;\n  select(!matches(\"shape|scale|prop_cens|tv_dist\"))\n\n# Define a violation threshold\nthreshold = 1e-04\n\nall_stats = res |&gt;\n  group_by(n) |&gt;\n  summarize(\n    total = n(),\n    SBS_median_n_violations = sum(SBS_median_diff &gt; threshold),\n    SBS_median_violation_rate = mean(SBS_median_diff &gt; threshold),\n    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff &gt; threshold]),\n    SBS_q10_n_violations = sum(SBS_q10_diff &gt; threshold),\n    SBS_q10_violation_rate = mean(SBS_q10_diff &gt; threshold),\n    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff &gt; threshold]),\n    SBS_q90_n_violations = sum(SBS_q90_diff &gt; threshold),\n    SBS_q90_violation_rate = mean(SBS_q90_diff &gt; threshold),\n    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff &gt; threshold]),\n    ISBS_n_violations = sum(ISBS_diff &gt; threshold),\n    ISBS_violation_rate = mean(ISBS_diff &gt; threshold),\n    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff &gt; threshold]),\n    RCLL_n_violations = sum(RCLL_diff &gt; threshold),\n    RCLL_violation_rate = mean(RCLL_diff &gt; threshold),\n    rRCLL_n_violations = sum(rRCLL_diff &gt; threshold),\n    rRCLL_violation_rate = mean(rRCLL_diff &gt; threshold),\n    .groups = \"drop\"\n  )\n\n\nNo violations for (r)RCLL:\n\n\nCode\nall_stats |&gt; \n  select(n | contains(\"RCLL\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE))\n\n\n\n\n\n\nViolation stats for SBS at different \\tau^*:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"SBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = c(4,7,10), digits = 5)\n\n\n\n\n\n\nViolation stats for ISBS:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"ISBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 4, digits = 5)\n\n\n\n\n\n\nLastly, we compare the average standard error of the scores across distributions (m=1000) with the mean score difference (only for simulations that had violations):\n\n\nCode\nmeasures = c(\"SBS_median\", \"SBS_q10\", \"SBS_q90\", \"ISBS\")\n\n# per measure\nse_res = lapply(measures, function(measure) {\n  diff_col = paste0(measure, \"_diff\")\n  se_col = paste0(measure, \"_se\")\n\n  res |&gt;\n    filter(.data[[diff_col]] &gt; threshold) |&gt; # only simulation with violations\n    mutate(se_diff = .data[[se_col]] &gt;= .data[[diff_col]]) |&gt; # do we have se &gt; delta_score?\n    group_by(n) |&gt;\n    summarise(prop_se_gte_diff = mean(se_diff), .groups = \"drop\") |&gt;\n    mutate(measure = measure)\n}) |&gt;\nbind_rows() |&gt;\npivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = \"prop_\")\n\nse_res |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 2:5, digits = 3)\n\n\n\n\n\n\nWe observe that for a significant proportion of simulations, the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference, providing evidence that some of these violations are a result of the variability of the scores."
  },
  {
    "objectID": "index.html#theoretical-vs-practical-properness",
    "href": "index.html#theoretical-vs-practical-properness",
    "title": "Supplementary Material",
    "section": "",
    "text": "Note\n\n\n\nThis sections refers to the results from the simulation experiments of Section 6.2 of the paper.\nUse properness_test.R to run the experiments. To ran the script for different sample sizes n, use the run_tests.sh script. The output results table (merged for all n) is available here.\n\n\nLoad libraries:\n\n\nCode\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(DT)\n\n\nLoad results, and show 10 randomly selected simulations outputs:\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_0.rds\")\n\nres[sample(1:nrow(res), 10), ] |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE)\n  ) |&gt;\n  formatRound(columns = 3:15, digits = c(rep(3,8), rep(4,5)))\n\n\n\n\n\n\nThe output columns are:\n\nsim: simulation number (k in the paper, in each simulation we draw m = 1000 times from each Weibull distribution)\nn: sample size (10 - 2500)\n{surv|pred|cens|}_{scale|shape} the scale and shape of the Weibull distributions for true, predicted and censoring times respectively\nprop_cens: proportion of censoring in each simulation\ntv_dist: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)\n{score}_diff: the mean score difference \\delta between true and predicted distribution. Scores used are the SBS at the 10, 50 and 90 quantiles of observed times, the ISBS from the 5 up to the 80 quantile (50 equidistant time points), the RCLL and the re-weighted rRCLL (RCLL** in the paper).\n{score}_se: the mean of two squared errors for the (I)SBS scores (using the true and the predicted survival distribution) across the m = 1000 distributions.\n\nCalculate violation stats:\n\n\nCode\nres = res |&gt;\n  select(!matches(\"shape|scale|prop_cens|tv_dist\"))\n\n# Define a violation threshold\nthreshold = 1e-04\n\nall_stats = res |&gt;\n  group_by(n) |&gt;\n  summarize(\n    total = n(),\n    SBS_median_n_violations = sum(SBS_median_diff &gt; threshold),\n    SBS_median_violation_rate = mean(SBS_median_diff &gt; threshold),\n    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff &gt; threshold]),\n    SBS_q10_n_violations = sum(SBS_q10_diff &gt; threshold),\n    SBS_q10_violation_rate = mean(SBS_q10_diff &gt; threshold),\n    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff &gt; threshold]),\n    SBS_q90_n_violations = sum(SBS_q90_diff &gt; threshold),\n    SBS_q90_violation_rate = mean(SBS_q90_diff &gt; threshold),\n    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff &gt; threshold]),\n    ISBS_n_violations = sum(ISBS_diff &gt; threshold),\n    ISBS_violation_rate = mean(ISBS_diff &gt; threshold),\n    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff &gt; threshold]),\n    RCLL_n_violations = sum(RCLL_diff &gt; threshold),\n    RCLL_violation_rate = mean(RCLL_diff &gt; threshold),\n    rRCLL_n_violations = sum(rRCLL_diff &gt; threshold),\n    rRCLL_violation_rate = mean(rRCLL_diff &gt; threshold),\n    .groups = \"drop\"\n  )\n\n\nNo violations for (r)RCLL:\n\n\nCode\nall_stats |&gt; \n  select(n | contains(\"RCLL\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE))\n\n\n\n\n\n\nViolation stats for SBS at different \\tau^*:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"SBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = c(4,7,10), digits = 5)\n\n\n\n\n\n\nViolation stats for ISBS:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"ISBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 4, digits = 5)\n\n\n\n\n\n\nLastly, we compare the average standard error of the scores across distributions (m=1000) with the mean score difference (only for simulations that had violations):\n\n\nCode\nmeasures = c(\"SBS_median\", \"SBS_q10\", \"SBS_q90\", \"ISBS\")\n\n# per measure\nse_res = lapply(measures, function(measure) {\n  diff_col = paste0(measure, \"_diff\")\n  se_col = paste0(measure, \"_se\")\n\n  res |&gt;\n    filter(.data[[diff_col]] &gt; threshold) |&gt; # only simulation with violations\n    mutate(se_diff = .data[[se_col]] &gt;= .data[[diff_col]]) |&gt; # do we have se &gt; delta_score?\n    group_by(n) |&gt;\n    summarise(prop_se_gte_diff = mean(se_diff), .groups = \"drop\") |&gt;\n    mutate(measure = measure)\n}) |&gt;\nbind_rows() |&gt;\npivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = \"prop_\")\n\nse_res |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 2:5, digits = 3)\n\n\n\n\n\n\nWe observe that for a significant proportion of simulations, the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference, providing evidence that some of these violations are a result of the variability of the scores."
  },
  {
    "objectID": "index.html#estimating-censoring",
    "href": "index.html#estimating-censoring",
    "title": "Supplementary Material",
    "section": "Estimating Censoring",
    "text": "Estimating Censoring\n\n\n\n\n\n\nNote\n\n\n\nThis sections refers to the results from the simulation experiments of Section 6.3 of the paper. The only difference with the previous section is that S_C(t) is now being estimated using the marginal Kaplan-Meier model via survival::survfit() instead of using the true Weibull censoring distribution (see helper.R. For SBS/ISBS we use constant interpolation of the censoring survival distribution S_C(t). For rRCLL we use linear interpolation of of S_C(t) to mitigate density estimation issues, i.e. for f_C(t).\nUse properness_test.R to run the experiments. To ran the script for different sample sizes n, use the run_tests.sh script changing to estimate_cens to TRUE. The output results table (merged for all n) is available here.\n\n\nLoad results (same output columns as in the previous section):\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_1.rds\") |&gt;\n  select(!matches(\"shape|scale|prop_cens|tv_dist\")) # remove columns\n\n\nCalculate violation stats:\n\n\nCode\nall_stats = res |&gt;\n  group_by(n) |&gt;\n  summarize(\n    total = n(),\n    SBS_median_n_violations = sum(SBS_median_diff &gt; threshold),\n    SBS_median_violation_rate = mean(SBS_median_diff &gt; threshold),\n    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff &gt; threshold]),\n    SBS_q10_n_violations = sum(SBS_q10_diff &gt; threshold),\n    SBS_q10_violation_rate = mean(SBS_q10_diff &gt; threshold),\n    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff &gt; threshold]),\n    SBS_q90_n_violations = sum(SBS_q90_diff &gt; threshold),\n    SBS_q90_violation_rate = mean(SBS_q90_diff &gt; threshold),\n    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff &gt; threshold]),\n    ISBS_n_violations = sum(ISBS_diff &gt; threshold),\n    ISBS_violation_rate = mean(ISBS_diff &gt; threshold),\n    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff &gt; threshold]),\n    RCLL_n_violations = sum(RCLL_diff &gt; threshold),\n    RCLL_violation_rate = mean(RCLL_diff &gt; threshold),\n    rRCLL_n_violations = sum(rRCLL_diff &gt; threshold),\n    rRCLL_violation_rate = mean(rRCLL_diff &gt; threshold),\n    .groups = \"drop\"\n  )\n\n\nNo violations for (r)RCLL:\n\n\nCode\nall_stats |&gt; \n  select(n | contains(\"RCLL\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE))\n\n\n\n\n\n\nViolation stats for SBS at different \\tau^*:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"SBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = c(4,7,10), digits = 5)\n\n\n\n\n\n\nViolation stats for ISBS:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"ISBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 4, digits = 5)\n\n\n\n\n\n\nWe compare the average standard error of the scores across distributions (m=1000) with the mean score difference (only for simulations that had violations):\n\n\nCode\nmeasures = c(\"SBS_median\", \"SBS_q10\", \"SBS_q90\", \"ISBS\")\n\n# per measure\nse_res = lapply(measures, function(measure) {\n  diff_col = paste0(measure, \"_diff\")\n  se_col = paste0(measure, \"_se\")\n\n  res |&gt;\n    filter(.data[[diff_col]] &gt; threshold) |&gt; # only simulation with violations\n    mutate(se_diff = .data[[se_col]] &gt;= .data[[diff_col]]) |&gt; # do we have se &gt; delta_score?\n    group_by(n) |&gt;\n    summarise(prop_se_gte_diff = mean(se_diff), .groups = \"drop\") |&gt;\n    mutate(measure = measure)\n}) |&gt;\nbind_rows() |&gt;\npivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = \"prop_\")\n\nse_res |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 2:5, digits = 3)\n\n\n\n\n\n\nWe again observe that for a significant proportion of simulations, the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference, providing evidence that some of these violations are a result of the variability of the scores."
  },
  {
    "objectID": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "href": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "title": "Supplementary Material",
    "section": "Degenerate Model Exploits ISBS Scoring Rule",
    "text": "Degenerate Model Exploits ISBS Scoring Rule\n\n\n\n\n\n\nNote\n\n\n\nThis section demonstrates how a deliberately simple, uninformative survival model—the degenerate model—can outperform established methods under the Integrated Survival Brier Score (ISBS). The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.\nWhile this model is intentionally unrealistic, its performance exposes a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility. This reinforces our theoretical findings that ISBS is not a proper scoring rule.\n\n\nBelow, we define the degenerate model in mlr3proba, tune it over the quantile cutoff with ISBS, and compare its ISBS score and other evaluation measures to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the survival::rats dataset.\nLoad libraries:\n\n\nCode\nlibrary(R6)\nlibrary(mlr3proba)\nlibrary(mlr3extralearners)\nlibrary(mlr3tuning)\n\n\nDefine the degenerate model:\n\n\nCode\nLearnerSurvDegenerate = R6Class(\"LearnerSurvDegenerate\",\n  inherit = LearnerSurv,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        id = \"surv.degenerate\",\n        predict_types = c(\"distr\"),\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"character\", \"factor\", \"ordered\"),\n        properties = \"missings\",\n        packages = c(\"survival\", \"distr6\"),\n        label = \"Degenerate Estimator\",\n        param_set = ps(\n           quantile = p_dbl(0, 1)\n        )\n      )\n    }\n  ),\n\n  private = list(\n    .train = function(task) {\n      list(time = task$truth()[,1L]) # store observed times\n    },\n\n    .predict = function(task) {\n      quantile_ps = self$param_set$values$quantile\n      times = sort(unique(self$model$time))\n      surv = matrix(nrow = task$nrow, ncol = length(times))\n\n      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]\n      \n      # same S for all test observations, sharp drop from 1 to 0 at q_t\n      surv[, 1:floor(q_t)] = 1\n      surv[, ceiling(q_t):ncol(surv)] = 0\n      colnames(surv) = times\n      .surv_return(times = times, surv = surv)\n    }\n  )\n)\n\n\nWe tune the degenerate model to find the optimal quantile to switch the prediction at:\n\n\nCode\nl = LearnerSurvDegenerate$new()\nl$param_set$values$quantile = to_tune(0, 1)\n\n# ISBS\nm = msr(\"surv.graf\")\n\n# Tune the quantile parameter of the degenerate model\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 20),\n  learner = l,\n  resampling = rsmp(\"holdout\"),\n  measure = m,\n)\n\n\nIn our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.\n\n\nCode\n# Seed for reproducibility\nset.seed(20250418)\n\n# Compare to Cox PH and Kaplan-Meier\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.rfsrc\")), at)\n\nr = rsmp(\"cv\", folds = 3)\nbm = benchmark(benchmark_grid(tasks = tsk(\"rats\"), learners = learners, resamplings = r))\n\n\nTo evaluate the benchmark we use Harrell’s C, D-calibration, RCLL, the SBS evaluated at three quantiles, and the ISBS:\n\n\nCode\nq25 = quantile(tsk(\"rats\")$times(), 0.25)\nq50 = quantile(tsk(\"rats\")$times(), 0.50)\nq75 = quantile(tsk(\"rats\")$times(), 0.75)\n\nmeasures = c(msrs(\n  c(\"surv.cindex\", \"surv.dcalib\", \"surv.rcll\")),\n  msr(\"surv.graf\", integrated = FALSE, times = q25, id = \"SBS_q25\"),\n  msr(\"surv.graf\", integrated = FALSE, times = q50, id = \"SBS_q0.5\"),\n  msr(\"surv.graf\", integrated = FALSE, times = q75, id = \"SBS_q0.75\"),\n  msr(\"surv.graf\", id = \"ISBS\")\n)\n\n\nFinally score results:\n\n\nCode\ndf = bm$aggregate(measures)[, c(4, 7:13)]\n\ndf |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE, order = list(list(7, \"desc\")))\n  ) |&gt;\n  formatRound(columns = 2:8, digits = 5) |&gt;\n  formatStyle(columns = 'ISBS', background = styleColorBar(df$ISBS, 'lightblue'))\n\n\n\n\n\n\nAnd we see the degenerate model performs the best with respect to ISBS and SBS at the 75% quantile of observed times, but worse with respect to all other measures."
  },
  {
    "objectID": "index.html#r-session-info",
    "href": "index.html#r-session-info",
    "title": "Supplementary Material",
    "section": "R session info",
    "text": "R session info\n\n\nCode\nutils::sessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 20.04.6 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_DK.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_DK.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_DK.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Oslo\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] mlr3tuning_1.2.1             paradox_1.0.1               \n [3] mlr3extralearners_1.0.0-9000 mlr3proba_0.7.4             \n [5] mlr3_0.23.0                  R6_2.5.1                    \n [7] DT_0.33                      tidyr_1.3.1                 \n [9] tibble_3.2.1                 dplyr_1.1.4                 \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6          xfun_0.49             bslib_0.8.0          \n [4] ggplot2_3.5.1         visNetwork_2.1.2      htmlwidgets_1.6.4    \n [7] lattice_0.22-6        vctrs_0.6.5           tools_4.4.2          \n[10] crosstalk_1.2.1       generics_0.1.3        parallel_4.4.2       \n[13] fansi_1.0.6           pkgconfig_2.0.3       Matrix_1.7-1         \n[16] data.table_1.16.4     checkmate_2.3.2       RColorBrewer_1.1-3   \n[19] randomForestSRC_3.3.3 mlr3pipelines_0.7.1   uuid_1.2-1           \n[22] lifecycle_1.0.4       compiler_4.4.2        set6_0.2.6           \n[25] munsell_0.5.1         data.tree_1.1.0       codetools_0.2-20     \n[28] bbotk_1.5.0           htmltools_0.5.8.1     sass_0.4.9           \n[31] yaml_2.3.10           pracma_2.4.4          pillar_1.9.0         \n[34] crayon_1.5.3          jquerylib_0.1.4       cachem_1.1.0         \n[37] parallelly_1.42.0     tidyselect_1.2.1      digest_0.6.37        \n[40] stringi_1.8.4         future_1.34.0         purrr_1.0.2          \n[43] listenv_0.9.1         splines_4.4.2         fastmap_1.2.0        \n[46] grid_4.4.2            colorspace_2.1-1      cli_3.6.3            \n[49] DiagrammeR_1.0.11     magrittr_2.0.3        survival_3.7-0       \n[52] utf8_1.2.4            future.apply_1.11.3   withr_3.0.2          \n[55] scales_1.3.0          backports_1.5.0       ooplah_0.2.0         \n[58] rmarkdown_2.29        globals_0.16.3        distr6_1.8.4         \n[61] evaluate_1.0.3        knitr_1.49            dictionar6_0.1.3     \n[64] mlr3misc_0.16.0       rlang_1.1.4           Rcpp_1.0.13-1        \n[67] glue_1.8.0            param6_0.2.4          palmerpenguins_0.1.1 \n[70] rstudioapi_0.17.1     jsonlite_1.8.9        lgr_0.4.4"
  }
]