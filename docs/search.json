[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Material",
    "section": "",
    "text": "Note\n\n\n\nThis sections refers to the results from the simulation experiments of Section 6.2 of the paper.\nUse properness_test.R to run the experiments. To ran the script for different sample sizes n, use the run_tests.sh script. The output results table (merged for all n) is available here.\n\n\nLoad libraries:\n\n\nCode\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(DT)\n\n\nLoad results, and show 10 randomly selected simulations outputs:\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_0.rds\")\n\nres[sample(1:nrow(res), 10), ] |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE)\n  ) |&gt;\n  formatRound(columns = 3:15, digits = c(rep(3,8), rep(4,5)))\n\n\n\n\n\n\nThe output columns are:\n\nsim: simulation number (k in the paper, in each simulation we draw m = 1000 times from each Weibull distribution)\nn: sample size (10 - 2500)\n{surv|pred|cens|}_{scale|shape} the scale and shape of the Weibull distributions for true, predicted and censoring times respectively\nprop_cens: proportion of censoring in each simulation\ntv_dist: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)\n{score}_diff: the mean score difference \\delta between true and predicted distribution. Scores used are the SBS at q10, q50 and q90 quantiles of observed times, the ISBS up to q80 quantile (50 equidistant time points), the RCLL and the re-weighted rRCLL (RCLL** in the paper).\n\nCalculate some stats:\n\n\nCode\nres = res |&gt;\n  select(!matches(\"shape|scale\")) |&gt; # remove columns\n  mutate(cens_bin = cut(prop_cens, breaks = seq(0, 1, by = 0.2), include.lowest = TRUE)) |&gt;\n  mutate(tv_dist_bin = cut(tv_dist, breaks = seq(0, 1, by = 0.25), include.lowest = TRUE)) |&gt;\n  select(!c(\"prop_cens\", \"tv_dist\"))\n\n# Define a violation threshold\nthreshold = 1e-04\n\n# all_stats = res |&gt;\n#   group_by(n) |&gt;\n#   summarize(\n#     total = n(),\n#     SBS_median_n_violations = sum(SBS_median_diff &gt; threshold),\n#     SBS_median_violation_rate = mean(SBS_median_diff &gt; threshold),\n#     SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff &gt; threshold]),\n#     SBS_q10_n_violations = sum(SBS_q10_diff &gt; threshold),\n#     SBS_q10_violation_rate = mean(SBS_q10_diff &gt; threshold),\n#     SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff &gt; threshold]),\n#     SBS_q90_n_violations = sum(SBS_q90_diff &gt; threshold),\n#     SBS_q90_violation_rate = mean(SBS_q90_diff &gt; threshold),\n#     SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff &gt; threshold]),\n#     ISBS_n_violations = sum(ISBS_diff &gt; threshold),\n#     ISBS_violation_rate = mean(ISBS_diff &gt; threshold),\n#     ISBS_diff_mean = mean(ISBS_diff[ISBS_diff &gt; threshold]),\n#     RCLL_n_violations = sum(RCLL_diff &gt; threshold),\n#     RCLL_violation_rate = mean(RCLL_diff &gt; threshold),\n#     rRCLL_n_violations = sum(rRCLL_diff &gt; threshold),\n#     rRCLL_violation_rate = mean(rRCLL_diff &gt; threshold),\n#     .groups = \"drop\"\n#   )"
  },
  {
    "objectID": "index.html#theoretical-vs-practical-properness",
    "href": "index.html#theoretical-vs-practical-properness",
    "title": "Supplementary Material",
    "section": "",
    "text": "Note\n\n\n\nThis sections refers to the results from the simulation experiments of Section 6.2 of the paper.\nUse properness_test.R to run the experiments. To ran the script for different sample sizes n, use the run_tests.sh script. The output results table (merged for all n) is available here.\n\n\nLoad libraries:\n\n\nCode\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(DT)\n\n\nLoad results, and show 10 randomly selected simulations outputs:\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_0.rds\")\n\nres[sample(1:nrow(res), 10), ] |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE)\n  ) |&gt;\n  formatRound(columns = 3:15, digits = c(rep(3,8), rep(4,5)))\n\n\n\n\n\n\nThe output columns are:\n\nsim: simulation number (k in the paper, in each simulation we draw m = 1000 times from each Weibull distribution)\nn: sample size (10 - 2500)\n{surv|pred|cens|}_{scale|shape} the scale and shape of the Weibull distributions for true, predicted and censoring times respectively\nprop_cens: proportion of censoring in each simulation\ntv_dist: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)\n{score}_diff: the mean score difference \\delta between true and predicted distribution. Scores used are the SBS at q10, q50 and q90 quantiles of observed times, the ISBS up to q80 quantile (50 equidistant time points), the RCLL and the re-weighted rRCLL (RCLL** in the paper).\n\nCalculate some stats:\n\n\nCode\nres = res |&gt;\n  select(!matches(\"shape|scale\")) |&gt; # remove columns\n  mutate(cens_bin = cut(prop_cens, breaks = seq(0, 1, by = 0.2), include.lowest = TRUE)) |&gt;\n  mutate(tv_dist_bin = cut(tv_dist, breaks = seq(0, 1, by = 0.25), include.lowest = TRUE)) |&gt;\n  select(!c(\"prop_cens\", \"tv_dist\"))\n\n# Define a violation threshold\nthreshold = 1e-04\n\n# all_stats = res |&gt;\n#   group_by(n) |&gt;\n#   summarize(\n#     total = n(),\n#     SBS_median_n_violations = sum(SBS_median_diff &gt; threshold),\n#     SBS_median_violation_rate = mean(SBS_median_diff &gt; threshold),\n#     SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff &gt; threshold]),\n#     SBS_q10_n_violations = sum(SBS_q10_diff &gt; threshold),\n#     SBS_q10_violation_rate = mean(SBS_q10_diff &gt; threshold),\n#     SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff &gt; threshold]),\n#     SBS_q90_n_violations = sum(SBS_q90_diff &gt; threshold),\n#     SBS_q90_violation_rate = mean(SBS_q90_diff &gt; threshold),\n#     SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff &gt; threshold]),\n#     ISBS_n_violations = sum(ISBS_diff &gt; threshold),\n#     ISBS_violation_rate = mean(ISBS_diff &gt; threshold),\n#     ISBS_diff_mean = mean(ISBS_diff[ISBS_diff &gt; threshold]),\n#     RCLL_n_violations = sum(RCLL_diff &gt; threshold),\n#     RCLL_violation_rate = mean(RCLL_diff &gt; threshold),\n#     rRCLL_n_violations = sum(rRCLL_diff &gt; threshold),\n#     rRCLL_violation_rate = mean(rRCLL_diff &gt; threshold),\n#     .groups = \"drop\"\n#   )"
  },
  {
    "objectID": "index.html#estimating-censoring",
    "href": "index.html#estimating-censoring",
    "title": "Supplementary Material",
    "section": "Estimating Censoring",
    "text": "Estimating Censoring\nThis sections shows the results from the simulation experiments of Section 6.3 of the paper. The only difference with the previous section is that S_C is now being estimated using the marginal Kaplan-Meier model via survival::survfit() instead of using the true Weibull censoring distribution.\nLoad results (same output columns as in the previous section), and show 10 randomly selected simulations outputs:\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_1.rds\")\n\nres[sample(1:nrow(res), 10), ] |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE)\n  ) |&gt;\n  formatRound(columns = 3:20, digits = c(rep(3,8), rep(4,10)))\n\n\n\n\n\n\nCalculate violation stats:\n\n\nCode\nres = res |&gt;\n  select(!matches(\"shape|scale\")) |&gt; # remove columns\n  mutate(cens_bin = cut(prop_cens, breaks = seq(0, 1, by = 0.2), include.lowest = TRUE)) |&gt;\n  mutate(tv_dist_bin = cut(tv_dist, breaks = seq(0, 1, by = 0.25), include.lowest = TRUE)) |&gt;\n  select(!c(\"prop_cens\", \"tv_dist\"))\n\nall_stats = res |&gt;\n  group_by(n) |&gt;\n  summarize(\n    total = n(),\n    SBS_median_n_violations = sum(SBS_median_diff &gt; threshold),\n    SBS_median_violation_rate = mean(SBS_median_diff &gt; threshold),\n    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff &gt; threshold]),\n    SBS_q10_n_violations = sum(SBS_q10_diff &gt; threshold),\n    SBS_q10_violation_rate = mean(SBS_q10_diff &gt; threshold),\n    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff &gt; threshold]),\n    SBS_q90_n_violations = sum(SBS_q90_diff &gt; threshold),\n    SBS_q90_violation_rate = mean(SBS_q90_diff &gt; threshold),\n    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff &gt; threshold]),\n    ISBS_n_violations = sum(ISBS_diff &gt; threshold),\n    ISBS_violation_rate = mean(ISBS_diff &gt; threshold),\n    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff &gt; threshold]),\n    RCLL_n_violations = sum(RCLL_diff &gt; threshold),\n    RCLL_violation_rate = mean(RCLL_diff &gt; threshold),\n    rRCLL_n_violations = sum(rRCLL_diff &gt; threshold),\n    rRCLL_violation_rate = mean(rRCLL_diff &gt; threshold),\n    .groups = \"drop\"\n  )\n\n\nNo violations for (r)RCLL:\n\n\nCode\nall_stats |&gt; \n  select(n | contains(\"RCLL\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE))\n\n\n\n\n\n\nViolation stats for SBS at different \\tau^*:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"SBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = c(4,7,10), digits = 5)\n\n\n\n\n\n\nViolation stats for ISBS:\n\n\nCode\nall_stats |&gt; \n  select(n | starts_with(\"ISBS\")) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 4, digits = 5)\n\n\n\n\n\n\nWe compare the average standard error of the scores across distributions (m=1000) with the mean score difference (only for simulations that had violations):\n\n\nCode\nmeasures = c(\"SBS_median\", \"SBS_q10\", \"SBS_q90\", \"ISBS\")\n\n# per measure\nse_res = lapply(measures, function(measure) {\n  diff_col = paste0(measure, \"_diff\")\n  se_col = paste0(measure, \"_se\")\n\n  res |&gt;\n    filter(.data[[diff_col]] &gt; threshold) |&gt; # only simulation with violations\n    mutate(se_diff = .data[[se_col]] &gt;= .data[[diff_col]]) |&gt; # do we have se &gt; delta_score?\n    group_by(n) |&gt;\n    summarise(prop_se_gte_diff = mean(se_diff), .groups = \"drop\") |&gt;\n    mutate(measure = measure)\n}) |&gt;\nbind_rows() |&gt;\npivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = \"prop_\")\n\nse_res |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 2:5, digits = 3)\n\n\n\n\n\n\nWe observe that for a significant proportion of simulations, the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference, providing evidence that some of these violations are a result of the variability of the scores."
  },
  {
    "objectID": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "href": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "title": "Supplementary Material",
    "section": "Degenerate Model Exploits ISBS Scoring Rule",
    "text": "Degenerate Model Exploits ISBS Scoring Rule\nThis section demonstrates how a deliberately simple, uninformative survival model—the degenerate model—can outperform established methods under the Integrated Survival Brier Score (ISBS). The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.\nWhile this model is intentionally unrealistic, its performance exposes a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility. This reinforces our theoretical findings that ISBS is not a proper scoring rule.\nBelow, we define the degenerate model in mlr3proba, tune it over the quantile cutoff with ISBS, and compare its ISBS score and other evaluation measures to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the survival::rats dataset.\nLoad libraries:\n\n\nCode\nlibrary(R6)\nlibrary(mlr3proba)\nlibrary(mlr3extralearners)\nlibrary(mlr3tuning)\n\n\nDefine the degenerate model:\n\n\nCode\nLearnerSurvDegenerate = R6Class(\"LearnerSurvDegenerate\",\n  inherit = LearnerSurv,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        id = \"surv.degenerate\",\n        predict_types = c(\"distr\"),\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"character\", \"factor\", \"ordered\"),\n        properties = \"missings\",\n        packages = c(\"survival\", \"distr6\"),\n        label = \"Degenerate Estimator\",\n        param_set = ps(\n           quantile = p_dbl(0, 1)\n        )\n      )\n    }\n  ),\n\n  private = list(\n    .train = function(task) {\n      list(time = task$truth()[,1L]) # store observed times\n    },\n\n    .predict = function(task) {\n      quantile_ps = self$param_set$values$quantile\n      times = sort(unique(self$model$time))\n      surv = matrix(nrow = task$nrow, ncol = length(times))\n\n      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]\n      \n      # same S for all test observations, sharp drop from 1 to 0 at q_t\n      surv[, 1:floor(q_t)] = 1\n      surv[, ceiling(q_t):ncol(surv)] = 0\n      colnames(surv) = times\n      .surv_return(times = times, surv = surv)\n    }\n  )\n)\n\n\nWe tune the degenerate model to find the optimal quantile to switch the prediction at:\n\n\nCode\nl = LearnerSurvDegenerate$new()\nl$param_set$values$quantile = to_tune(0, 1)\n\n# ISBS\nm = msr(\"surv.graf\")\n\n# Tune the quantile parameter of the degenerate model\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 20),\n  learner = l,\n  resampling = rsmp(\"holdout\"),\n  measure = m,\n)\n\n\nIn our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.\n\n\nCode\n# Seed for reproducibility\nset.seed(20250418)\n\n# Compare to Cox PH and Kaplan-Meier\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.rfsrc\")), at)\n\nr = rsmp(\"cv\", folds = 3)\nbm = benchmark(benchmark_grid(tasks = tsk(\"rats\"), learners = learners, resamplings = r))\n\n\nTo evaluate the benchmark we use Harrell’s C, D-calibration, RCLL, the SBS evaluated at three quantiles, and the ISBS:\n\n\nCode\nq25 = quantile(tsk(\"rats\")$times(), 0.25)\nq50 = quantile(tsk(\"rats\")$times(), 0.50)\nq75 = quantile(tsk(\"rats\")$times(), 0.75)\n\nmeasures = c(msrs(\n  c(\"surv.cindex\", \"surv.dcalib\", \"surv.rcll\")),\n  msr(\"surv.graf\", integrated = FALSE, times = q25, id = \"SBS_q25\"),\n  msr(\"surv.graf\", integrated = FALSE, times = q50, id = \"SBS_q0.5\"),\n  msr(\"surv.graf\", integrated = FALSE, times = q75, id = \"SBS_q0.75\"),\n  msr(\"surv.graf\", id = \"ISBS\")\n)\n\n\nFinally score results:\n\n\nCode\nbm$aggregate(measures)[, c(4, 7:13)]\n\n\n              learner_id surv.cindex  surv.dcalib surv.rcll    SBS_q25\n                  &lt;char&gt;       &lt;num&gt;        &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            surv.coxph   0.7637313 4.629203e-01  3.626923 0.07522030\n2:           surv.kaplan   0.5000000 9.691005e-01  3.676610 0.07700273\n3:            surv.rfsrc   0.7516121 8.259461e-01  3.634021 0.07576037\n4: surv.degenerate.tuned   0.5000000 1.283224e+10 17.039130 0.08399497\n    SBS_q0.5 SBS_q0.75       ISBS\n       &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1: 0.1091271  2.068774 0.06850171\n2: 0.1212525  2.327660 0.07341427\n3: 0.1084741  1.649696 0.06500299\n4: 0.1400008  0.000000 0.06306716\n\n\nAnd we see the degenerate model performs the best with respect to ISBS and SBS at the 75% quantile of observed times, but worse with respect to all other measures."
  }
]