[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Material",
    "section": "",
    "text": "To illustrate the diversity of the Weibull distributions used in our simulation design, we plot both the density f(t) and survival S(t) functions for 10 randomly sampled shape \\alpha and scale \\sigma parameter pairs. These distributions represent either the true survival, predicted survival, or censoring times across simulations. The wide parameter range induces substantial variability, ensuring a rich family of distributions and increasing the sensitivity of our evaluation to detect violations of properness. By construction, all data-generating mechanisms assume Y independent of C, satisfying the random censoring assumption.\n\n\nCode\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\nset.seed(20250408)\n\nshape = runif(10, 0.5, 5)\nscale = runif(10, 0.5, 5)\n\ndw = mapply(function(shape,scale) dweibull(seq.int(1,5,0.1), shape,scale),shape,scale) |&gt;\n  reshape2::melt() |&gt;\n  mutate(x = rep(seq.int(1,5,0.1), 10),\n        group = rep(sprintf(\"(%.2f,%.2f)\",shape,scale), each = 41)) |&gt;\n  select(x, group, value)\npw = mapply(function(shape,scale) pweibull(seq.int(1,5,0.1), shape,scale, lower.tail=F),shape,scale) |&gt;\n  reshape2::melt() |&gt;\n  mutate(x = rep(seq.int(1,5,0.1), 10),\n        group = rep(sprintf(\"(%.2f,%.2f)\",shape,scale), each = 41)) |&gt;\n  select(x, group, value)\n\n(ggplot(dw, aes(x = x, y = value, group = group, color = group)) + ylab(\"f(x)\")) /\n(ggplot(pw, aes(x = x, y = value, group = group, color = group)) + ylab(\"S(x)\")) +\n  plot_layout(guides='collect') &\n  geom_line() &\n  theme_classic() &\n  guides(color = guide_legend(title='(α,σ)')) &\n  scale_color_viridis_d(option = \"turbo\")\n\n\n\n\n\nWeibull density (top) and survival (bottom) curves for randomly generated shape, α, and scale, σ parameters. Each curve corresponds to a different parameter pair, illustrating the broad variability across simulated distributions.\n\n\n\n\nCode\n# ggsave(\"weibull_plots.png\", device = png, dpi=600, width =7, height = 4)"
  },
  {
    "objectID": "index.html#high-variability-for-generated-survival-and-censoring-distributions",
    "href": "index.html#high-variability-for-generated-survival-and-censoring-distributions",
    "title": "Supplementary Material",
    "section": "",
    "text": "To illustrate the diversity of the Weibull distributions used in our simulation design, we plot both the density f(t) and survival S(t) functions for 10 randomly sampled shape \\alpha and scale \\sigma parameter pairs. These distributions represent either the true survival, predicted survival, or censoring times across simulations. The wide parameter range induces substantial variability, ensuring a rich family of distributions and increasing the sensitivity of our evaluation to detect violations of properness. By construction, all data-generating mechanisms assume Y independent of C, satisfying the random censoring assumption.\n\n\nCode\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\nset.seed(20250408)\n\nshape = runif(10, 0.5, 5)\nscale = runif(10, 0.5, 5)\n\ndw = mapply(function(shape,scale) dweibull(seq.int(1,5,0.1), shape,scale),shape,scale) |&gt;\n  reshape2::melt() |&gt;\n  mutate(x = rep(seq.int(1,5,0.1), 10),\n        group = rep(sprintf(\"(%.2f,%.2f)\",shape,scale), each = 41)) |&gt;\n  select(x, group, value)\npw = mapply(function(shape,scale) pweibull(seq.int(1,5,0.1), shape,scale, lower.tail=F),shape,scale) |&gt;\n  reshape2::melt() |&gt;\n  mutate(x = rep(seq.int(1,5,0.1), 10),\n        group = rep(sprintf(\"(%.2f,%.2f)\",shape,scale), each = 41)) |&gt;\n  select(x, group, value)\n\n(ggplot(dw, aes(x = x, y = value, group = group, color = group)) + ylab(\"f(x)\")) /\n(ggplot(pw, aes(x = x, y = value, group = group, color = group)) + ylab(\"S(x)\")) +\n  plot_layout(guides='collect') &\n  geom_line() &\n  theme_classic() &\n  guides(color = guide_legend(title='(α,σ)')) &\n  scale_color_viridis_d(option = \"turbo\")\n\n\n\n\n\nWeibull density (top) and survival (bottom) curves for randomly generated shape, α, and scale, σ parameters. Each curve corresponds to a different parameter pair, illustrating the broad variability across simulated distributions.\n\n\n\n\nCode\n# ggsave(\"weibull_plots.png\", device = png, dpi=600, width =7, height = 4)"
  },
  {
    "objectID": "index.html#table-d1",
    "href": "index.html#table-d1",
    "title": "Supplementary Material",
    "section": "Table D1",
    "text": "Table D1\n\n\n\n\n\n\nNote\n\n\n\nThis section supports the “Simulation Experiments” section of the paper.\nUse properness_test.R to run the experiments. To run for different sample sizes n, use run_tests.sh. Merged simulation results for all n are available here.\n\n\nLoad libraries and explore data:\n\n\nCode\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(DT)\n\nres = readRDS(\"results/res_sims10000_distrs1000_0.rds\")\n\nres[sample(1:nrow(res), 10), ] |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 3:22, digits = c(rep(3,8), rep(5,12)))\n\n\n\n\n\n\nEach row corresponds to one simulation, using m = 1000 draws from Weibull survival distributions at a fixed sample size n \\in {10, \\dots, 1000}.\nThe output columns are:\n\nsim: simulation index (k in the paper)\nn: sample size\n{surv|pred|cens|}_{scale|shape} the scale and shape of the Weibull distributions for true, predicted and censoring times respectively\nprop_cens: proportion of censoring in each simulation\ntv_dist: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)\n{score}_diff, {score}_sd: the mean and standard deviation of the score difference D between true and predicted distributions across draws\n\nScoring rules analyzed:\n\nThe SBS at the 10th, 50th and 90th percentiles of observed times\nThe ISBS, using 50 equidistant points between the 5th and 80th percentile of observed times\nThe RCLL and its re-weighted rRCLL (RCLL* in the paper)\n\nWe compute 95% confidence intervals (CIs) for the mean score difference D using a t-distribution. A violation is marked statistically significant if:\n\nThe mean score difference exceeds a threshold: \\bar{D} &gt; 0.001\nThe CI excludes zero (i.e., \\text{CI}_\\text{lower} &gt; 0)\n\n\n\nCode\nres = res |&gt;\n  select(!matches(\"shape|scale|prop_cens|tv_dist\"))\n\nmeasures = c(\"SBS_median\", \"SBS_q10\", \"SBS_q90\", \"ISBS\", \"RCLL\", \"rRCLL\")\nn_distrs = 1000 # `m` value from paper's experiment\ntcrit = qt(0.975, df = n_distrs - 1) # 95% t-test\nthreshold = 1e-3\n\ndata = measures |&gt;\n  lapply(function(m) {\n    mean_diff_col = paste0(m, \"_diff\")\n    mean_sd_col = paste0(m, \"_sd\")\n    \n    res |&gt;\n      select(sim, n, !!mean_diff_col, !!mean_sd_col) |&gt;\n      mutate(\n        se = .data[[mean_sd_col]] / sqrt(n_distrs),\n        CI_lower = .data[[mean_diff_col]] - tcrit * se,\n        CI_upper = .data[[mean_diff_col]] + tcrit * se,\n        signif_violation = .data[[mean_diff_col]] &gt; threshold & CI_lower &gt; 0\n      ) |&gt;\n      select(!se) |&gt;\n      mutate(metric = !!m) |&gt;\n      relocate(metric, .after = 1) |&gt;\n      rename(diff = !!mean_diff_col,\n             sd   = !!mean_sd_col)\n  }) |&gt;\n  bind_rows()\n\ndata$metric = factor(\n  data$metric,\n  levels = measures,\n  labels = c(\"SBS (Median)\", \"SBS (Q10)\", \"SBS (Q90)\", \"ISBS\", \"RCLL\", \"RCLL*\")\n)\n\n\nFor example, the violation results for the first simulation and n = 250 across all scoring rules are:\n\n\nCode\ndata |&gt; \n  filter(sim == 1, n == 250) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 4:7, digits = 5)\n\n\n\n\n\n\nWe summarize violations across simulations, by computing for each score & sample size:\n\nNumber of significant violations\nViolation rate\nAverage, median, min and max score difference among simulations where violations occurred\n\n\n\nCode\nall_stats = \n  data |&gt; \n  group_by(n, metric) |&gt; \n  summarize(\n    n_violations = sum(signif_violation),\n    violation_rate = mean(signif_violation),\n    diff_mean = if (any(signif_violation)) mean(diff[signif_violation]) else NA_real_,\n    diff_median = if (any(signif_violation)) median(diff[signif_violation]) else NA_real_,\n    diff_min = if (any(signif_violation)) min(diff[signif_violation]) else NA_real_,\n    diff_max = if (any(signif_violation)) max(diff[signif_violation]) else NA_real_,\n    .groups = \"drop\"\n  )\n\nall_stats |&gt;\n  arrange(metric) |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE), \n    caption = htmltools::tags$caption(\n      style = \"caption-side: top; text-align: center; font-size:150%\",\n      \"Table D1: Empirical violations of properness\")\n  ) |&gt;\n  formatRound(columns = 4:8, digits = 5)\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\nRCLL and RCLL* show no violations across any simulation or sample size.\nSBS shows time- and sample-size-dependent violations, mostly at smaller n, especially for early evaluation times (\\tau = q_{0.1}). All differences are small (typically &lt; 0.01).\nISBS showed minor violations only at n = 10; none for n &gt; 50."
  },
  {
    "objectID": "index.html#table-d2",
    "href": "index.html#table-d2",
    "title": "Supplementary Material",
    "section": "Table D2",
    "text": "Table D2\n\n\n\n\n\n\nNote\n\n\n\nThe only difference with the previous experiment is that S_C(t) is now being estimated using the marginal Kaplan-Meier model via survival::survfit() instead of using the true Weibull censoring distribution (see helper.R). For SBS/ISBS we use constant interpolation of the censoring survival distribution S_C(t). For RCLL* we use linear interpolation of S_C(t) to mitigate density estimation issues, i.e. for f_C(t).\nUse properness_test.R to run the experiments. To run for different sample sizes n, use run_tests.sh, changing estimate_cens to TRUE. Merged simulation results for all n are available here.\n\n\nLoad results (same output columns as in the previous section):\n\n\nCode\nres = readRDS(\"results/res_sims10000_distrs1000_1.rds\") |&gt;\n  select(!matches(\"shape|scale|prop_cens|tv_dist\")) # remove columns\n\n\nAs before, we compute 95% confidence intervals for the score differences across m = 1000 draws per simulation:\n\n\nCode\nmeasures = c(\"SBS_median\", \"SBS_q10\", \"SBS_q90\", \"ISBS\", \"RCLL\", \"rRCLL\")\nn_distrs = 1000 # `m` value from paper's experiment\ntcrit = qt(0.975, df = n_distrs - 1) # 95% t-test\nthreshold = 1e-3\n\ndata = measures |&gt;\n  lapply(function(m) {\n    mean_diff_col = paste0(m, \"_diff\")\n    mean_sd_col = paste0(m, \"_sd\")\n    \n    res |&gt;\n      select(sim, n, !!mean_diff_col, !!mean_sd_col) |&gt;\n      mutate(\n        se = .data[[mean_sd_col]] / sqrt(n_distrs),\n        CI_lower = .data[[mean_diff_col]] - tcrit * se,\n        CI_upper = .data[[mean_diff_col]] + tcrit * se,\n        signif_violation = .data[[mean_diff_col]] &gt; threshold & CI_lower &gt; 0\n      ) |&gt;\n      select(!se) |&gt;\n      mutate(metric = !!m) |&gt;\n      relocate(metric, .after = 1) |&gt;\n      rename(diff = !!mean_diff_col,\n             sd   = !!mean_sd_col)\n  }) |&gt;\n  bind_rows()\n\ndata$metric = factor(\n  data$metric,\n  levels = measures,\n  labels = c(\"SBS (Median)\", \"SBS (Q10)\", \"SBS (Q90)\", \"ISBS\", \"RCLL\", \"RCLL*\")\n)\n\n\nLastly, we summarize the significant violations across sample sizes and scoring rules:\n\n\nCode\nall_stats = \n  data |&gt; \n  group_by(n, metric) |&gt; \n  summarize(\n    n_violations = sum(signif_violation),\n    violation_rate = mean(signif_violation),\n    diff_mean = if (any(signif_violation)) mean(diff[signif_violation]) else NA_real_,\n    diff_median = if (any(signif_violation)) median(diff[signif_violation]) else NA_real_,\n    diff_min = if (any(signif_violation)) min(diff[signif_violation]) else NA_real_,\n    diff_max = if (any(signif_violation)) max(diff[signif_violation]) else NA_real_,\n    .groups = \"drop\"\n  )\n\nall_stats |&gt;\n  arrange(metric) |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE), \n    caption = htmltools::tags$caption(\n      style = \"caption-side: top; text-align: center; font-size:150%\",\n      \"Table D2: Empirical violations of properness using G(t)\")\n  ) |&gt;\n  formatRound(columns = 4:8, digits = 5)\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nEstimating S_C(t) via Kaplan-Meier produced nearly identical results to using the true distribution, with no violations for RCLL/RCLL* and only small-sample effects for SBS and ISBS."
  },
  {
    "objectID": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "href": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "title": "Supplementary Material",
    "section": "Degenerate Model Exploits ISBS Scoring Rule",
    "text": "Degenerate Model Exploits ISBS Scoring Rule\n\n\n\n\n\n\nNote\n\n\n\nThis section demonstrates how a deliberately simple, uninformative survival model—the degenerate model—can outperform established methods under the Integrated Survival Brier Score (ISBS). The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.\nThis model is deliberately constructed to expose a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility. This reinforces our theoretical findings that ISBS is not a proper scoring rule.\n\n\nBelow, we define the degenerate model in mlr3proba, tune it over the quantile cutoff with ISBS, and compare its ISBS score and other evaluation measures to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the survival::rats dataset.\nLoad libraries:\n\n\nCode\nlibrary(R6)\nlibrary(mlr3proba)\nlibrary(mlr3extralearners)\nlibrary(mlr3tuning)\n\n\nDefine the degenerate model:\n\n\nCode\nLearnerSurvDegenerate = R6Class(\"LearnerSurvDegenerate\",\n  inherit = LearnerSurv,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        id = \"surv.degenerate\",\n        predict_types = c(\"distr\"),\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"character\", \"factor\", \"ordered\"),\n        properties = \"missings\",\n        packages = c(\"survival\", \"distr6\"),\n        label = \"Degenerate Estimator\",\n        param_set = ps(\n           quantile = p_dbl(0, 1)\n        )\n      )\n    }\n  ),\n\n  private = list(\n    .train = function(task) {\n      list(time = task$truth()[,1L]) # store observed times\n    },\n\n    .predict = function(task) {\n      quantile_ps = self$param_set$values$quantile\n      times = sort(unique(self$model$time))\n      surv = matrix(nrow = task$nrow, ncol = length(times))\n\n      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]\n      \n      # same S for all test observations, sharp drop from 1 to 0 at q_t\n      surv[, 1:floor(q_t)] = 1\n      surv[, ceiling(q_t):ncol(surv)] = 0\n      colnames(surv) = times\n      .surv_return(times = times, surv = surv)\n    }\n  )\n)\n\n\nWe tune the degenerate model to find the optimal quantile to switch the prediction at:\n\n\nCode\nl = LearnerSurvDegenerate$new()\nl$param_set$values$quantile = to_tune(0, 1)\n\n# ISBS\nm = msr(\"surv.graf\")\n\n# Tune the quantile parameter of the degenerate model\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 20),\n  learner = l,\n  resampling = rsmp(\"holdout\"),\n  measure = m,\n)\n\n\nIn our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.\n\n\nCode\n# Seed for reproducibility\nset.seed(20250418)\n\n# Compare to Cox PH and Kaplan-Meier\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.rfsrc\")), at)\n\nr = rsmp(\"cv\", folds = 3)\nbm = benchmark(benchmark_grid(tasks = tsk(\"rats\"), learners = learners, resamplings = r))\n\n\nTo evaluate the benchmark we use Harrell’s C, D-calibration, RCLL, the SBS evaluated at the median observed time, and the ISBS:\n\n\nCode\nq50 = quantile(tsk(\"rats\")$times(), 0.50)\n\nmeasures = c(msrs(\n  c(\"surv.cindex\", \"surv.dcalib\", \"surv.rcll\")),\n  msr(\"surv.graf\", integrated = FALSE, times = q50, id = \"SBS_q0.5\"),\n  msr(\"surv.graf\", id = \"ISBS\")\n)\n\n\nFinally score results:\n\n\nCode\ndf = bm$aggregate(measures)[, c(4, 7:11)]\n\ndf |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    options = list(searching = FALSE, order = list(list(5, \"desc\")))\n  ) |&gt;\n  formatRound(columns = c(2, 4, 5, 6), digits = 5) |&gt;\n  formatSignif(columns = 3, digits = 3) # scientific notation for surv.dcalib\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nAnd we see the degenerate model performs the best with respect to ISBS, but worse with respect to all other measures."
  },
  {
    "objectID": "index.html#real-world-benchmark-evaluation",
    "href": "index.html#real-world-benchmark-evaluation",
    "title": "Supplementary Material",
    "section": "Real-World Benchmark Evaluation",
    "text": "Real-World Benchmark Evaluation\n\n\n\n\n\n\nNote\n\n\n\nTo run the experiment use rrcll_benchmark.R.\n\n\nWe assess the performance of several survival scoring rules (RCLL, RCLL*, ISBS, D-calibration, Harrell’s C-index) on real-world survival datasets using the mlr3proba framework, tested with 10 repetitions of 5-fold cross-validation on the following learners:\n\nKaplan-Meier (non-parametric)\nCox proportional hazards model\nRandom survival forests (RSF)\n\n\n\nCode\n# (normal) RCLL\nrcll = msr(\"surv.rrcll\", id = \"RCLL\", proper = FALSE)\n# (re-weighted) rRCLL/RCLL*\nrrcll = msr(\"surv.rrcll\", id = \"RRCLL\", proper = TRUE)\n\n# get all survival tasks in mlr3proba\nkeys = as.data.table(mlr_tasks)[task_type == \"surv\"][[\"key\"]]\ntasks = lapply(keys, function(key) {\n  tsk(key)\n})\n\nset.seed(42)\nbm_grid = benchmark_grid(\n  tasks = tasks,\n  learners = lrns(c(\"surv.kaplan\", \"surv.ranger\", \"surv.coxph\")),\n  resamplings = rsmp(\"repeated_cv\", folds = 5)\n)\n\nbm = benchmark(bm_grid)\nres = bm$aggregate(\n  measures = c(rcll, rrcll, msr(\"surv.cindex\"), msr(\"surv.dcalib\"), msr(\"surv.graf\"))\n)\n\n\nBelow we show the aggregated performance metrics across all tasks and learners:\n\n\nCode\nres = readRDS(\"results/rrcll_bm.rds\")\n\nres |&gt;\n  rename(`RCLL*` = RRCLL) |&gt;\n  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |&gt;\n  formatRound(columns = 3:7, digits = 3) |&gt;\n  formatSignif(columns = 6, digits = 3) # scientific notation for surv.dcalib\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nOur experiment demonstrates substantial divergences in model assessment depending on the chosen metric. Although RCLL* is the only proper scoring rule in a theoretical sense, its practical behavior differs meaningfully from other commonly used metrics.\nFor instance, on datasets such as veteran and whas, which exhibit non-proportional hazards, RSF is more suitable to model non-linear effects compared to the CoxPH. In these cases, RCLL* identifies RSF as superior, whereas RCLL and ISBS still prefer CoxPH — suggesting potential practical limitations of the latter metrics in such settings.\nAdditionally, RCLL* appears to better capture both discrimination and calibration. In the gbcs dataset, CoxPH achieves the best RCLL, ISBS and C-index scores, yet shows extremely poor calibration (\\text{D-calib} \\approx 15.7), while RCLL* ranks Kaplan-Meier as the best — possibly reflecting better-calibrated risk estimates. This discrepancy becomes even more pronounced in the lung dataset, where CoxPH again yields high RCLL/C-index/ISBS performance despite severe calibration failure (\\text{D-calib} \\approx 23.5), whereas RCLL* selects RSF instead.\nThese findings highlight a critical need for further research: not all metrics labeled as “proper” in theory behave equally in practice."
  },
  {
    "objectID": "index.html#r-session-info",
    "href": "index.html#r-session-info",
    "title": "Supplementary Material",
    "section": "R session info",
    "text": "R session info\n\n\nCode\nutils::sessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 20.04.6 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_DK.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_DK.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_DK.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Oslo\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] mlr3tuning_1.2.1             paradox_1.0.1               \n [3] mlr3extralearners_1.0.0-9000 mlr3proba_0.7.4             \n [5] mlr3_0.23.0                  R6_2.5.1                    \n [7] DT_0.33                      tidyr_1.3.1                 \n [9] tibble_3.2.1                 dplyr_1.1.4                 \n[11] patchwork_1.3.0              ggplot2_3.5.1               \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6          xfun_0.49             bslib_0.8.0          \n [4] visNetwork_2.1.2      htmlwidgets_1.6.4     lattice_0.22-6       \n [7] vctrs_0.6.5           tools_4.4.2           crosstalk_1.2.1      \n[10] generics_0.1.3        parallel_4.4.2        fansi_1.0.6          \n[13] pkgconfig_2.0.3       Matrix_1.7-1          data.table_1.16.4    \n[16] checkmate_2.3.2       RColorBrewer_1.1-3    randomForestSRC_3.3.3\n[19] mlr3pipelines_0.7.1   uuid_1.2-1            lifecycle_1.0.4      \n[22] set6_0.2.6            compiler_4.4.2        farver_2.1.2         \n[25] stringr_1.5.1         munsell_0.5.1         data.tree_1.1.0      \n[28] codetools_0.2-20      bbotk_1.5.0           htmltools_0.5.8.1    \n[31] sass_0.4.9            yaml_2.3.10           pracma_2.4.4         \n[34] pillar_1.9.0          crayon_1.5.3          jquerylib_0.1.4      \n[37] cachem_1.1.0          parallelly_1.42.0     tidyselect_1.2.1     \n[40] digest_0.6.37         stringi_1.8.4         future_1.34.0        \n[43] reshape2_1.4.4        purrr_1.0.2           listenv_0.9.1        \n[46] splines_4.4.2         labeling_0.4.3        fastmap_1.2.0        \n[49] grid_4.4.2            colorspace_2.1-1      cli_3.6.3            \n[52] DiagrammeR_1.0.11     magrittr_2.0.3        survival_3.7-0       \n[55] utf8_1.2.4            future.apply_1.11.3   withr_3.0.2          \n[58] scales_1.3.0          backports_1.5.0       ooplah_0.2.0         \n[61] rmarkdown_2.29        globals_0.16.3        distr6_1.8.4         \n[64] evaluate_1.0.3        knitr_1.49            viridisLite_0.4.2    \n[67] dictionar6_0.1.3      mlr3misc_0.16.0       rlang_1.1.4          \n[70] Rcpp_1.0.13-1         glue_1.8.0            param6_0.2.4         \n[73] palmerpenguins_0.1.1  rstudioapi_0.17.1     jsonlite_1.8.9       \n[76] lgr_0.4.4             plyr_1.8.9"
  }
]