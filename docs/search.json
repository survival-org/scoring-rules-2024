[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Material",
    "section": "",
    "text": "This section demonstrates how a deliberately simple, uninformative survival model—the degenerate model—can outperform established methods under the Integrated Survival Brier Score (ISBS). The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.\nWhile this model is intentionally unrealistic, its performance exposes a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility. This reinforces our theoretical findings that ISBS is not a proper scoring rule.\nBelow, we define the degenerate model in mlr3proba, tune it over the quantile cutoff, and compare its ISBS score to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the survival::rats dataset.\nLoad libraries:\n\n\nCode\nlibrary(R6)\nlibrary(mlr3proba)\nlibrary(mlr3extralearners)\nlibrary(mlr3tuning)\n\n\nDefine the degenerate model:\n\n\nCode\nLearnerSurvDegenerate = R6Class(\"LearnerSurvDegenerate\",\n  inherit = LearnerSurv,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        id = \"surv.degenerate\",\n        predict_types = c(\"distr\"),\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"character\", \"factor\", \"ordered\"),\n        properties = \"missings\",\n        packages = c(\"survival\", \"distr6\"),\n        label = \"Degenerate Estimator\",\n        param_set = ps(\n           quantile = p_dbl(0, 1)\n        )\n      )\n    }\n  ),\n\n  private = list(\n    .train = function(task) {\n      list(time = task$truth()[,1L]) # store observed times\n    },\n\n    .predict = function(task) {\n      quantile_ps = self$param_set$values$quantile\n      times = sort(unique(self$model$time))\n      surv = matrix(nrow = task$nrow, ncol = length(times))\n\n      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]\n      \n      # same S for all test observations, sharp drop from 1 to 0 at q_t\n      surv[, 1:floor(q_t)] = 1\n      surv[, ceiling(q_t):ncol(surv)] = 0\n      colnames(surv) = times\n      .surv_return(times = times, surv = surv)\n    }\n  )\n)\n\n\nWe tune the degenerate model to find the optimal quantile to switch the prediction at:\n\n\nCode\nl = LearnerSurvDegenerate$new()\nl$param_set$values$quantile = to_tune(0, 1)\n\n# ISBS\nm = msr(\"surv.graf\")\n\n# Tune the quantile parameter of the degenerate model\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 20),\n  learner = l,\n  resampling = rsmp(\"holdout\"),\n  measure = m,\n)\n\n\nIn our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.\n\n\nCode\n# Seed for reproducibility\nset.seed(20250418)\n\n# Compare to Cox PH and Kaplan-Meier\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.rfsrc\")), at)\n\nr = rsmp(\"cv\", folds = 3)\nbm = benchmark(benchmark_grid(tasks = tsk(\"rats\"), learners = learners, resamplings = r))\n\n\nFinally score results:\n\n\nCode\nbm$aggregate(m)[,c(4,7)]\n\n\n              learner_id  surv.graf\n                  &lt;char&gt;      &lt;num&gt;\n1:            surv.coxph 0.06850171\n2:           surv.kaplan 0.07341427\n3:            surv.rfsrc 0.06500299\n4: surv.degenerate.tuned 0.06306716\n\n\nAnd we see the degenerate model performs the best."
  },
  {
    "objectID": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "href": "index.html#degenerate-model-exploits-isbs-scoring-rule",
    "title": "Supplementary Material",
    "section": "",
    "text": "This section demonstrates how a deliberately simple, uninformative survival model—the degenerate model—can outperform established methods under the Integrated Survival Brier Score (ISBS). The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.\nWhile this model is intentionally unrealistic, its performance exposes a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility. This reinforces our theoretical findings that ISBS is not a proper scoring rule.\nBelow, we define the degenerate model in mlr3proba, tune it over the quantile cutoff, and compare its ISBS score to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the survival::rats dataset.\nLoad libraries:\n\n\nCode\nlibrary(R6)\nlibrary(mlr3proba)\nlibrary(mlr3extralearners)\nlibrary(mlr3tuning)\n\n\nDefine the degenerate model:\n\n\nCode\nLearnerSurvDegenerate = R6Class(\"LearnerSurvDegenerate\",\n  inherit = LearnerSurv,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        id = \"surv.degenerate\",\n        predict_types = c(\"distr\"),\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"character\", \"factor\", \"ordered\"),\n        properties = \"missings\",\n        packages = c(\"survival\", \"distr6\"),\n        label = \"Degenerate Estimator\",\n        param_set = ps(\n           quantile = p_dbl(0, 1)\n        )\n      )\n    }\n  ),\n\n  private = list(\n    .train = function(task) {\n      list(time = task$truth()[,1L]) # store observed times\n    },\n\n    .predict = function(task) {\n      quantile_ps = self$param_set$values$quantile\n      times = sort(unique(self$model$time))\n      surv = matrix(nrow = task$nrow, ncol = length(times))\n\n      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]\n      \n      # same S for all test observations, sharp drop from 1 to 0 at q_t\n      surv[, 1:floor(q_t)] = 1\n      surv[, ceiling(q_t):ncol(surv)] = 0\n      colnames(surv) = times\n      .surv_return(times = times, surv = surv)\n    }\n  )\n)\n\n\nWe tune the degenerate model to find the optimal quantile to switch the prediction at:\n\n\nCode\nl = LearnerSurvDegenerate$new()\nl$param_set$values$quantile = to_tune(0, 1)\n\n# ISBS\nm = msr(\"surv.graf\")\n\n# Tune the quantile parameter of the degenerate model\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 20),\n  learner = l,\n  resampling = rsmp(\"holdout\"),\n  measure = m,\n)\n\n\nIn our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.\n\n\nCode\n# Seed for reproducibility\nset.seed(20250418)\n\n# Compare to Cox PH and Kaplan-Meier\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.rfsrc\")), at)\n\nr = rsmp(\"cv\", folds = 3)\nbm = benchmark(benchmark_grid(tasks = tsk(\"rats\"), learners = learners, resamplings = r))\n\n\nFinally score results:\n\n\nCode\nbm$aggregate(m)[,c(4,7)]\n\n\n              learner_id  surv.graf\n                  &lt;char&gt;      &lt;num&gt;\n1:            surv.coxph 0.06850171\n2:           surv.kaplan 0.07341427\n3:            surv.rfsrc 0.06500299\n4: surv.degenerate.tuned 0.06306716\n\n\nAnd we see the degenerate model performs the best."
  }
]