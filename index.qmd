---
title: "Supplementary Material"
author: "[John Zobolas](https://github.com/bblodfon)"
date: last-modified
description: "Supporting information for paper 'Examining  properness in the external validation of survival models with squared and logarithmic losses'"
format:
  html:
    date: last-modified
    code-block-bg: true
    code-copy: true
    code-fold: true
    code-overflow: wrap
    code-block-border-left: true
    toc: true
    toc-location: left
    html-math-method: katex
    page-layout: full
execute:
  freeze: true
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

## Theoretical vs Practical Properness {-}

:::{.callout-note}
This sections refers to the results from the simulation experiments of Section 6.2 of the paper.

Use [properness_test.R](https://github.com/survival-org/scoring-rules-2024/blob/main/properness_test.R) to run the experiments.
To ran the script for different sample sizes `n`, use the [run_tests.sh](https://github.com/survival-org/scoring-rules-2024/blob/main/run_tests.sh) script.
The output results table (merged for all `n`) is available [here](https://github.com/survival-org/scoring-rules-2024/blob/main/results/res_sims10000_distrs1000_0.rds).
:::

Load libraries:
```{r, message=FALSE}
library(dplyr)
library(tibble)
library(tidyr)
library(DT)
```

Load results, and show 10 randomly selected simulations outputs:
```{r}
res = readRDS("results/res_sims10000_distrs1000_0.rds")

res[sample(1:nrow(res), 10), ] |>
  DT::datatable(
    rownames = FALSE,
    options = list(searching = FALSE)
  ) |>
  formatRound(columns = 3:15, digits = c(rep(3,8), rep(4,5)))
```

The output columns are:

- `sim`: simulation number ($k$ in the paper, in each simulation we draw $m = 1000$ times from each Weibull distribution)
- `n`: sample size ($10 - 2500$)
- `{surv|pred|cens|}_{scale|shape}` the scale and shape of the Weibull distributions for true, predicted and censoring times respectively
- `prop_cens`: proportion of censoring in each simulation
- `tv_dist`: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)
- `{score}_diff`: the mean score difference $\delta$ between true and predicted distribution. Scores used are the **SBS** at q10, q50 and q90 quantiles of observed times, the **ISBS** up to q80 quantile (50 equidistant time points), the **RCLL** and the **re-weighted rRCLL** (RCLL\*\* in the paper).
- `{score}_se`: the mean of two squared errors for the (I)SBS scores (using the true and the predicted survival distribution) across the $m = 1000$ distributions.

Calculate violation stats:
```{r}
res = res |>
  select(!matches("shape|scale|prop_cens|tv_dist"))

# Define a violation threshold
threshold = 1e-04

all_stats = res |>
  group_by(n) |>
  summarize(
    total = n(),
    SBS_median_n_violations = sum(SBS_median_diff > threshold),
    SBS_median_violation_rate = mean(SBS_median_diff > threshold),
    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff > threshold]),
    SBS_q10_n_violations = sum(SBS_q10_diff > threshold),
    SBS_q10_violation_rate = mean(SBS_q10_diff > threshold),
    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff > threshold]),
    SBS_q90_n_violations = sum(SBS_q90_diff > threshold),
    SBS_q90_violation_rate = mean(SBS_q90_diff > threshold),
    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff > threshold]),
    ISBS_n_violations = sum(ISBS_diff > threshold),
    ISBS_violation_rate = mean(ISBS_diff > threshold),
    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff > threshold]),
    RCLL_n_violations = sum(RCLL_diff > threshold),
    RCLL_violation_rate = mean(RCLL_diff > threshold),
    rRCLL_n_violations = sum(rRCLL_diff > threshold),
    rRCLL_violation_rate = mean(rRCLL_diff > threshold),
    .groups = "drop"
  )
```

No violations for (r)RCLL:
```{r}
all_stats |> 
  select(n | contains("RCLL")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE))
```

Violation stats for SBS at different $\tau^*$:
```{r}
all_stats |> 
  select(n | starts_with("SBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = c(4,7,10), digits = 5)
```

Violation stats for ISBS:
```{r}
all_stats |> 
  select(n | starts_with("ISBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 4, digits = 5)
```

Lastly, we compare the average standard error of the scores across distributions ($m=1000$) with the mean score difference (only for simulations that had violations):
```{r}
measures = c("SBS_median", "SBS_q10", "SBS_q90", "ISBS")

# per measure
se_res = lapply(measures, function(measure) {
  diff_col = paste0(measure, "_diff")
  se_col = paste0(measure, "_se")

  res |>
    filter(.data[[diff_col]] > threshold) |> # only simulation with violations
    mutate(se_diff = .data[[se_col]] >= .data[[diff_col]]) |> # do we have se > delta_score?
    group_by(n) |>
    summarise(prop_se_gte_diff = mean(se_diff), .groups = "drop") |>
    mutate(measure = measure)
}) |>
bind_rows() |>
pivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = "prop_")

se_res |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 2:5, digits = 3)
```

We observe that for a significant proportion of simulations, **the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference**, providing evidence that some of these violations are a result of the variability of the scores.

## Estimating Censoring {-}

:::{.callout-note}
This sections refers to the results from the simulation experiments of Section 6.3 of the paper.
The only difference with the previous section is that $S_C(t)$ is now being estimated using the marginal Kaplan-Meier model via `survival::survfit()` instead of using the true Weibull censoring distribution (see [helper.R](https://github.com/survival-org/scoring-rules-2024/blob/main/helper.R).
For SBS/ISBS we use constant interpolation of the censoring survival distribution $S_C(t)$.
For rRCLL we use linear interpolation of of $S_C(t)$ to mitigate density estimation issues, i.e. for $f_C(t)$.

Use [properness_test.R](https://github.com/survival-org/scoring-rules-2024/blob/main/properness_test.R) to run the experiments.
To ran the script for different sample sizes `n`, use the [run_tests.sh](https://github.com/survival-org/scoring-rules-2024/blob/main/run_tests.sh) script changing to `estimate_cens` to `TRUE`.
The output results table (merged for all `n`) is available [here](https://github.com/survival-org/scoring-rules-2024/blob/main/results/res_sims10000_distrs1000_1.rds).
:::

Load results (same output columns as in the previous section):
```{r}
res = readRDS("results/res_sims10000_distrs1000_1.rds") |>
  select(!matches("shape|scale|prop_cens|tv_dist")) # remove columns
```

Calculate violation stats:
```{r}
all_stats = res |>
  group_by(n) |>
  summarize(
    total = n(),
    SBS_median_n_violations = sum(SBS_median_diff > threshold),
    SBS_median_violation_rate = mean(SBS_median_diff > threshold),
    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff > threshold]),
    SBS_q10_n_violations = sum(SBS_q10_diff > threshold),
    SBS_q10_violation_rate = mean(SBS_q10_diff > threshold),
    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff > threshold]),
    SBS_q90_n_violations = sum(SBS_q90_diff > threshold),
    SBS_q90_violation_rate = mean(SBS_q90_diff > threshold),
    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff > threshold]),
    ISBS_n_violations = sum(ISBS_diff > threshold),
    ISBS_violation_rate = mean(ISBS_diff > threshold),
    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff > threshold]),
    RCLL_n_violations = sum(RCLL_diff > threshold),
    RCLL_violation_rate = mean(RCLL_diff > threshold),
    rRCLL_n_violations = sum(rRCLL_diff > threshold),
    rRCLL_violation_rate = mean(rRCLL_diff > threshold),
    .groups = "drop"
  )
```

No violations for (r)RCLL:
```{r}
all_stats |> 
  select(n | contains("RCLL")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE))
```

Violation stats for SBS at different $\tau^*$:
```{r}
all_stats |> 
  select(n | starts_with("SBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = c(4,7,10), digits = 5)
```

Violation stats for ISBS:
```{r}
all_stats |> 
  select(n | starts_with("ISBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 4, digits = 5)
```

We compare the average standard error of the scores across distributions ($m=1000$) with the mean score difference (only for simulations that had violations):
```{r}
measures = c("SBS_median", "SBS_q10", "SBS_q90", "ISBS")

# per measure
se_res = lapply(measures, function(measure) {
  diff_col = paste0(measure, "_diff")
  se_col = paste0(measure, "_se")

  res |>
    filter(.data[[diff_col]] > threshold) |> # only simulation with violations
    mutate(se_diff = .data[[se_col]] >= .data[[diff_col]]) |> # do we have se > delta_score?
    group_by(n) |>
    summarise(prop_se_gte_diff = mean(se_diff), .groups = "drop") |>
    mutate(measure = measure)
}) |>
bind_rows() |>
pivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = "prop_")

se_res |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 2:5, digits = 3)
```

We again observe that for a significant proportion of simulations, **the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference**, providing evidence that some of these violations are a result of the variability of the scores.

## Degenerate Model Exploits ISBS Scoring Rule {-}

:::{.callout-note}
This section demonstrates how a deliberately simple, uninformative survival model—the *degenerate model*—can outperform established methods under the Integrated Survival Brier Score (ISBS).
The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.

While this model is intentionally unrealistic, its performance exposes a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility.
This reinforces our theoretical findings that ISBS is not a proper scoring rule.
:::

Below, we define the degenerate model in `mlr3proba`, tune it over the quantile cutoff with ISBS, and compare its ISBS score and other evaluation measures to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the `survival::rats` dataset.

Load libraries:
```{r, message=FALSE}
library(R6)
library(mlr3proba)
library(mlr3extralearners)
library(mlr3tuning)
```

Define the degenerate model:
```{r}
LearnerSurvDegenerate = R6Class("LearnerSurvDegenerate",
  inherit = LearnerSurv,
  public = list(
    initialize = function() {
      super$initialize(
        id = "surv.degenerate",
        predict_types = c("distr"),
        feature_types = c("logical", "integer", "numeric", "character", "factor", "ordered"),
        properties = "missings",
        packages = c("survival", "distr6"),
        label = "Degenerate Estimator",
        param_set = ps(
           quantile = p_dbl(0, 1)
        )
      )
    }
  ),

  private = list(
    .train = function(task) {
      list(time = task$truth()[,1L]) # store observed times
    },

    .predict = function(task) {
      quantile_ps = self$param_set$values$quantile
      times = sort(unique(self$model$time))
      surv = matrix(nrow = task$nrow, ncol = length(times))

      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]
      
      # same S for all test observations, sharp drop from 1 to 0 at q_t
      surv[, 1:floor(q_t)] = 1
      surv[, ceiling(q_t):ncol(surv)] = 0
      colnames(surv) = times
      .surv_return(times = times, surv = surv)
    }
  )
)
```

We tune the degenerate model to find the optimal quantile to switch the prediction at:
```{r}
l = LearnerSurvDegenerate$new()
l$param_set$values$quantile = to_tune(0, 1)

# ISBS
m = msr("surv.graf")

# Tune the quantile parameter of the degenerate model
at = auto_tuner(
  tuner = tnr("grid_search", resolution = 20),
  learner = l,
  resampling = rsmp("holdout"),
  measure = m,
)
```

In our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.
```{r, results='hide'}
# Seed for reproducibility
set.seed(20250418)

# Compare to Cox PH and Kaplan-Meier
learners = c(lrns(c("surv.coxph", "surv.kaplan", "surv.rfsrc")), at)

r = rsmp("cv", folds = 3)
bm = benchmark(benchmark_grid(tasks = tsk("rats"), learners = learners, resamplings = r))
```

To evaluate the benchmark we use Harrell’s C, D-calibration, RCLL, the SBS evaluated at three quantiles, and the ISBS:
```{r}
q25 = quantile(tsk("rats")$times(), 0.25)
q50 = quantile(tsk("rats")$times(), 0.50)
q75 = quantile(tsk("rats")$times(), 0.75)

measures = c(msrs(
  c("surv.cindex", "surv.dcalib", "surv.rcll")),
  msr("surv.graf", integrated = FALSE, times = q25, id = "SBS_q25"),
  msr("surv.graf", integrated = FALSE, times = q50, id = "SBS_q0.5"),
  msr("surv.graf", integrated = FALSE, times = q75, id = "SBS_q0.75"),
  msr("surv.graf", id = "ISBS")
)
```

Finally score results:
```{r}
bm$aggregate(measures)[, c(4, 7:13)] |>
  DT::datatable(
    rownames = FALSE,
    options = list(searching = FALSE)
  ) |>
  formatRound(columns = 2:8, digits = 5)
```

And we see the degenerate model performs the best with respect to ISBS and SBS at the 75% quantile of observed times, but worse with respect to all other measures.

## R session info {-}

```{r}
utils::sessionInfo()
```
