---
title: "Supplementary Material"
author: "[John Zobolas](https://github.com/bblodfon)"
date: last-modified
description: "Supporting information for paper 'Examining  properness in the external validation of survival models with squared and logarithmic losses'"
format:
  html:
    date: last-modified
    code-block-bg: true
    code-copy: true
    code-fold: true
    code-overflow: wrap
    code-block-border-left: true
    toc: true
    toc-location: left
    html-math-method: katex
    page-layout: full
execute:
  freeze: true
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

## High variability for generated survival and censoring distributions {-}

To illustrate the diversity of the Weibull distributions used in our simulation design, we plot both the density $f(t)$ and survival $S(t)$ functions for 10 randomly sampled shape $\alpha$ and scale $\sigma$ parameter pairs.
These distributions represent either the true survival, predicted survival, or censoring times across simulations.
The wide parameter range induces substantial variability, ensuring a rich family of distributions and increasing the sensitivity of our evaluation to detect violations of properness.
By construction, all data-generating mechanisms assume $Y$ independent of $C$, satisfying the random censoring assumption.

```{r, message=FALSE}
#| fig-cap: "Weibull density (top) and survival (bottom) curves for randomly generated shape, α, and scale, σ parameters. Each curve corresponds to a different parameter pair, illustrating the broad variability across simulated distributions."

library(ggplot2)
library(patchwork)
library(dplyr)

set.seed(20250408)

shape = runif(10, 0.5, 5)
scale = runif(10, 0.5, 5)

dw = mapply(function(shape,scale) dweibull(seq.int(1,5,0.1), shape,scale),shape,scale) %>%
  reshape2::melt() %>%
  mutate(x = rep(seq.int(1,5,0.1), 10),
        group = rep(sprintf("(%.2f,%.2f)",shape,scale), each = 41)) %>%
  select(x, group, value)
pw = mapply(function(shape,scale) pweibull(seq.int(1,5,0.1), shape,scale, lower.tail=F),shape,scale) %>%
  reshape2::melt() %>%
  mutate(x = rep(seq.int(1,5,0.1), 10),
        group = rep(sprintf("(%.2f,%.2f)",shape,scale), each = 41)) %>%
  select(x, group, value)

(ggplot(dw, aes(x = x, y = value, group = group, color = group)) + ylab("f(x)")) /
  (ggplot(pw, aes(x = x, y = value, group = group, color = group)) + ylab("S(x)")) +
  plot_layout(guides='collect') &
  geom_line() &
  theme_classic() &
  guides(color = guide_legend(title='(α,σ)'))

# ggsave("weibull_plots.png", device = png, dpi=600, width =7, height = 4)
```

## Theoretical vs Practical Properness {-}

:::{.callout-note}
This sections refers to the results from the simulation experiments of Section 6.2 of the paper.

Use [properness_test.R](https://github.com/survival-org/scoring-rules-2024/blob/main/properness_test.R) to run the experiments.
To ran the script for different sample sizes `n`, use the [run_tests.sh](https://github.com/survival-org/scoring-rules-2024/blob/main/run_tests.sh) script.
The output results table (merged for all `n`) is available [here](https://github.com/survival-org/scoring-rules-2024/blob/main/results/res_sims10000_distrs1000_0.rds).
:::

Load libraries:
```{r, message=FALSE}
library(dplyr)
library(tibble)
library(tidyr)
library(DT)
```

Load results, and show 10 randomly selected simulations outputs:
```{r}
res = readRDS("results/res_sims10000_distrs1000_0.rds")

res[sample(1:nrow(res), 10), ] |>
  DT::datatable(
    rownames = FALSE,
    options = list(searching = FALSE)
  ) |>
  formatRound(columns = 3:20, digits = c(rep(3,8), rep(5,10)))
```

The output columns are:

- `sim`: simulation number ($k$ in the paper, in each simulation we draw $m = 1000$ times from each Weibull distribution)
- `n`: sample size ($10 - 2000$)
- `{surv|pred|cens|}_{scale|shape}` the scale and shape of the Weibull distributions for true, predicted and censoring times respectively
- `prop_cens`: proportion of censoring in each simulation
- `tv_dist`: the total variation distance between the true and the predicted Weibull distribution (closer to 0 means more similar distributions)
- `{score}_diff`: the mean score difference $\delta$ between true and predicted distribution. Scores used are the **SBS** at the 10, 50 and 90 quantiles of observed times, the **ISBS** from the 5 up to the 80 quantile (50 equidistant time points), the **RCLL** and the **re-weighted rRCLL** (RCLL\*\* in the paper).
- `{score}_se`: the mean of two squared errors for the (I)SBS scores across the $m = 1000$ distributions (one is when using the true and the other the predicted survival distribution).

Calculate violation stats:
```{r}
res = res |>
  select(!matches("shape|scale|prop_cens|tv_dist"))

# Define a violation threshold
threshold = 1e-04

all_stats = res |>
  group_by(n) |>
  summarize(
    total = n(),
    SBS_median_n_violations = sum(SBS_median_diff > threshold),
    SBS_median_violation_rate = mean(SBS_median_diff > threshold),
    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff > threshold]),
    SBS_q10_n_violations = sum(SBS_q10_diff > threshold),
    SBS_q10_violation_rate = mean(SBS_q10_diff > threshold),
    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff > threshold]),
    SBS_q90_n_violations = sum(SBS_q90_diff > threshold),
    SBS_q90_violation_rate = mean(SBS_q90_diff > threshold),
    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff > threshold]),
    ISBS_n_violations = sum(ISBS_diff > threshold),
    ISBS_violation_rate = mean(ISBS_diff > threshold),
    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff > threshold]),
    RCLL_n_violations = sum(RCLL_diff > threshold),
    RCLL_violation_rate = mean(RCLL_diff > threshold),
    rRCLL_n_violations = sum(rRCLL_diff > threshold),
    rRCLL_violation_rate = mean(rRCLL_diff > threshold),
    .groups = "drop"
  )
```

No violations for (r)RCLL:
```{r}
all_stats |> 
  select(n | contains("RCLL")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE))
```

Violation stats for SBS at different $\tau^*$:
```{r}
all_stats |> 
  select(n | starts_with("SBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = c(4,7,10), digits = 5)
```

Violation stats for ISBS:
```{r}
all_stats |> 
  select(n | starts_with("ISBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 4, digits = 5)
```

To better understand the nature of observed violations, we assess whether they may arise from the inherent variability in score estimates across repeated draws.
Specifically, for each scoring rule (SBS and ISBS), we compute the proportion of violating simulations in which the standard error (SE) of the score across the $m = 1000$ distributions exceeds the mean score difference observed in that same simulation:

```{r}
measures = c("SBS_median", "SBS_q10", "SBS_q90", "ISBS")

# per measure
se_res = lapply(measures, function(measure) {
  diff_col = paste0(measure, "_diff")
  se_col = paste0(measure, "_se")

  res |>
    filter(.data[[diff_col]] > threshold) |> # only simulation with violations
    mutate(se_diff = .data[[se_col]] >= .data[[diff_col]]) |> # do we have se > delta_score?
    group_by(n) |>
    summarise(prop_se_gte_diff = mean(se_diff), .groups = "drop") |>
    mutate(measure = measure)
}) |>
bind_rows() |>
pivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = "prop_")

se_res |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 2:5, digits = 3)
```

We observe that for a significant proportion of simulations, **the standard error of the SBS/ISBS losses across distributions is as large or larger than the observed mean score difference**.
This supports the interpretation that many violations may stem from statistical noise rather than genuine failures of properness.

## Estimating Censoring {-}

:::{.callout-note}
This sections refers to the results from the simulation experiments of Section 6.3 of the paper.
The only difference with the previous section is that $S_C(t)$ is now being estimated using the marginal Kaplan-Meier model via `survival::survfit()` instead of using the true Weibull censoring distribution (see [helper.R](https://github.com/survival-org/scoring-rules-2024/blob/main/helper.R).
For SBS/ISBS we use constant interpolation of the censoring survival distribution $S_C(t)$.
For rRCLL we use linear interpolation of of $S_C(t)$ to mitigate density estimation issues, i.e. for $f_C(t)$.

Use [properness_test.R](https://github.com/survival-org/scoring-rules-2024/blob/main/properness_test.R) to run the experiments.
To ran the script for different sample sizes `n`, use the [run_tests.sh](https://github.com/survival-org/scoring-rules-2024/blob/main/run_tests.sh) script changing to `estimate_cens` to `TRUE`.
The output results table (merged for all `n`) is available [here](https://github.com/survival-org/scoring-rules-2024/blob/main/results/res_sims10000_distrs1000_1.rds).
:::

Load results (same output columns as in the previous section):
```{r}
res = readRDS("results/res_sims10000_distrs1000_1.rds") |>
  select(!matches("shape|scale|prop_cens|tv_dist")) # remove columns
```

Calculate violation stats:
```{r}
# Define a violation threshold
threshold = 1e-04

all_stats = res |>
  group_by(n) |>
  summarize(
    total = n(),
    SBS_median_n_violations = sum(SBS_median_diff > threshold),
    SBS_median_violation_rate = mean(SBS_median_diff > threshold),
    SBS_median_diff_mean = mean(SBS_median_diff[SBS_median_diff > threshold]),
    SBS_q10_n_violations = sum(SBS_q10_diff > threshold),
    SBS_q10_violation_rate = mean(SBS_q10_diff > threshold),
    SBS_q10_diff_mean = mean(SBS_q10_diff[SBS_q10_diff > threshold]),
    SBS_q90_n_violations = sum(SBS_q90_diff > threshold),
    SBS_q90_violation_rate = mean(SBS_q90_diff > threshold),
    SBS_q90_diff_mean = mean(SBS_q90_diff[SBS_q90_diff > threshold]),
    ISBS_n_violations = sum(ISBS_diff > threshold),
    ISBS_violation_rate = mean(ISBS_diff > threshold),
    ISBS_diff_mean = mean(ISBS_diff[ISBS_diff > threshold]),
    RCLL_n_violations = sum(RCLL_diff > threshold),
    RCLL_violation_rate = mean(RCLL_diff > threshold),
    rRCLL_n_violations = sum(rRCLL_diff > threshold),
    rRCLL_violation_rate = mean(rRCLL_diff > threshold),
    .groups = "drop"
  )
```

No violations for (r)RCLL:
```{r}
all_stats |> 
  select(n | contains("RCLL")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE))
```

Violation stats for SBS at different $\tau^*$:
```{r}
all_stats |> 
  select(n | starts_with("SBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = c(4,7,10), digits = 5)
```

Violation stats for ISBS:
```{r}
all_stats |> 
  select(n | starts_with("ISBS")) |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 4, digits = 5)
```

As in the previous experiment, we assess whether violations could be explained by variability in score estimates.
For each scoring rule, we again compute the proportion of violating simulations where the standard error exceeds the observed mean score difference:
```{r}
measures = c("SBS_median", "SBS_q10", "SBS_q90", "ISBS")

# per measure
se_res = lapply(measures, function(measure) {
  diff_col = paste0(measure, "_diff")
  se_col = paste0(measure, "_se")

  res |>
    filter(.data[[diff_col]] > threshold) |> # only simulation with violations
    mutate(se_diff = .data[[se_col]] >= .data[[diff_col]]) |> # do we have se > delta_score?
    group_by(n) |>
    summarise(prop_se_gte_diff = mean(se_diff), .groups = "drop") |>
    mutate(measure = measure)
}) |>
bind_rows() |>
pivot_wider(names_from = measure, values_from = prop_se_gte_diff, names_prefix = "prop_")

se_res |>
  DT::datatable(rownames = FALSE, options = list(searching = FALSE)) |>
  formatRound(columns = 2:5, digits = 3)
```

We again observe that for a significant proportion of simulations, **the SE of the SBS/ISBS losses across distributions is larger or equal to the mean score difference**, providing evidence that some of these violations are a result of the variability of the scores.

## Degenerate Model Exploits ISBS Scoring Rule {-}

:::{.callout-note}
This section demonstrates how a deliberately simple, uninformative survival model—the *degenerate model*—can outperform established methods under the Integrated Survival Brier Score (ISBS).
The model entirely ignores covariates and instead outputs a flat survival function that drops from 1 to 0 at a fixed quantile of observed times.

This model is deliberately constructed to expose a key weakness in ISBS: it can be minimized by predictions that offer no individualization or clinical utility.
This reinforces our theoretical findings that ISBS is not a proper scoring rule.
:::

Below, we define the degenerate model in `mlr3proba`, tune it over the quantile cutoff with ISBS, and compare its ISBS score and other evaluation measures to Cox, Kaplan-Meier, and Random Survival Forest (RSF) learners on the `survival::rats` dataset.

Load libraries:
```{r, message=FALSE}
library(R6)
library(mlr3proba)
library(mlr3extralearners)
library(mlr3tuning)
```

Define the degenerate model:
```{r}
LearnerSurvDegenerate = R6Class("LearnerSurvDegenerate",
  inherit = LearnerSurv,
  public = list(
    initialize = function() {
      super$initialize(
        id = "surv.degenerate",
        predict_types = c("distr"),
        feature_types = c("logical", "integer", "numeric", "character", "factor", "ordered"),
        properties = "missings",
        packages = c("survival", "distr6"),
        label = "Degenerate Estimator",
        param_set = ps(
           quantile = p_dbl(0, 1)
        )
      )
    }
  ),

  private = list(
    .train = function(task) {
      list(time = task$truth()[,1L]) # store observed times
    },

    .predict = function(task) {
      quantile_ps = self$param_set$values$quantile
      times = sort(unique(self$model$time))
      surv = matrix(nrow = task$nrow, ncol = length(times))

      q_t = quantile(seq.int(length(times)), quantile_ps)[[1]]
      
      # same S for all test observations, sharp drop from 1 to 0 at q_t
      surv[, 1:floor(q_t)] = 1
      surv[, ceiling(q_t):ncol(surv)] = 0
      colnames(surv) = times
      .surv_return(times = times, surv = surv)
    }
  )
)
```

We tune the degenerate model to find the optimal quantile to switch the prediction at:
```{r}
l = LearnerSurvDegenerate$new()
l$param_set$values$quantile = to_tune(0, 1)

# ISBS
m = msr("surv.graf")

# Tune the quantile parameter of the degenerate model
at = auto_tuner(
  tuner = tnr("grid_search", resolution = 20),
  learner = l,
  resampling = rsmp("holdout"),
  measure = m,
)
```

In our benchmark experiment we compare to the Cox PH, Random Forest, and Kaplan-Meier using 3-fold outer cross-validation.
```{r, results='hide'}
# Seed for reproducibility
set.seed(20250418)

# Compare to Cox PH and Kaplan-Meier
learners = c(lrns(c("surv.coxph", "surv.kaplan", "surv.rfsrc")), at)

r = rsmp("cv", folds = 3)
bm = benchmark(benchmark_grid(tasks = tsk("rats"), learners = learners, resamplings = r))
```

To evaluate the benchmark we use Harrell’s C, D-calibration, RCLL, the SBS evaluated at the median observed time, and the ISBS:
```{r}
q50 = quantile(tsk("rats")$times(), 0.50)

measures = c(msrs(
  c("surv.cindex", "surv.dcalib", "surv.rcll")),
  msr("surv.graf", integrated = FALSE, times = q50, id = "SBS_q0.5"),
  msr("surv.graf", id = "ISBS")
)
```

Finally score results:
```{r}
df = bm$aggregate(measures)[, c(4, 7:11)]

df |>
  DT::datatable(
    rownames = FALSE,
    options = list(searching = FALSE, order = list(list(5, "desc")))
  ) |>
  formatRound(columns = c(2, 4, 5, 6), digits = 5) |>
  formatSignif(columns = 3, digits = 3) # scientific notation for surv.dcalib
```

And we see the degenerate model performs the best with respect to ISBS and SBS at the 75% quantile of observed times, but worse with respect to all other measures.

## R session info {-}

```{r}
utils::sessionInfo()
```
